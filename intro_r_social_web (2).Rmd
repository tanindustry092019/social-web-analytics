---
title: "Intro to R" 
subtitle: "Social web analytics"
author: "Dr. Le Nhat Tan"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
 rmdformats::readthedown:
    thumbnails: true
    lightbox: true
    gallery: true
    highlight: tango
    use_bookdown: true
---


```{r setup,include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,fig.align = 'center',out.width = '90%')
library(knitr)
#opts_knit$set(root.dir = "C:\\Users\\Asus\\Dropbox\\giao an DHQT")
library(igraph)
```

# Graphs

##  Creating a graph

### From data frame


```{r}
edge=read.csv("Social network analysis.csv")
edge
```


```{r}
vertex = read.csv("Social network analysis_Vertex.csv")
vertex
```



```{r}
g1 <- graph_from_data_frame(d = edge, vertices = vertex, directed = FALSE)
plot(g1)
```
```{r}
# Set vertex color by gender
V(g1)$color <- ifelse(V(g1)$Age >=20, "orange", "dodgerblue")
E(g1)$width <- edge$weekly_talk_hours
# Plot the graph
plot(g1, vertex.label.color = "black")

```



```{r}
m1 <- layout_nicely(g1)
plot(g1, vertex.label.color = "black", layout = m1)

```







### From graph formula

We can create a graph by providing the graph.formula function with the set of vertices, and how they are connected (edges). For example:

```{r}
g1 = graph.formula(A - B, A - C, A - D, B - D)
```


```{r}
plot(g1)
```

### From adjacency matrix

```{r}
(A=matrix(c(0,1,1,1,1,0,0,1, 1,0,0,0, 1,1,0,0),byrow=TRUE, nrow=4))
g2 = graph.adjacency(A,mode="undirected")
plot(g2)

```


### From  Edge List

```{r}
el = matrix(c("A", "A", "A", "B", "B", "C", "D", "D"), 4, 2)
print(el)
```
```{r}
g4 = graph.edgelist(el, directed = FALSE)
plot(g4)
```

### Random graph

```{r}
g.er = erdos.renyi.game(n = 50, p = 0.1)
plot(g.er, vertex.size = 5)
```

```{r}
g.er = erdos.renyi.game(n = 150, p = 0.1)
plot(g.er, vertex.size = 1)
```

###  Barabási–Albert Graph

To create a Barabási–Albert Graph, we must provide 
n
 (the number of vertices). We can also provide the 
k
 (the power) and 
m
 (the number of edges to add to each new vertex).
 
```{r}
g.ba = barabasi.game(n = 50, directed = FALSE)
plot(g.ba, vertex.size = 5)

```
 



##  Examining the Graphs

###  Density
By visually examining the two graphs above, which looks denser? Use the function graph.density to compute the density of each graph and compare the results to your guess.


```{r}
g.er = erdos.renyi.game(n = 7,p = 0.7)
plot(g.er,vertex.size=1)

```

```{r}
g.ba = barabasi.game(n = 7, directed=FALSE)
plot(g.ba,vertex.size=1)
```


```{r}
graph.density(g.er)
graph.density(g.ba)
```
###  Diameter

The diameter is the longest shortest path. Which of the two graphs do you expect to have the largest diameter? Use the function diameter to compute the diameter of each graph.


```{r}
diameter(g.er)
diameter(g.ba)
```

###  Degree
What do you expect the degree distribution of each graph to look like? We can compute the degree of each vertex using the function degree. We can also compute the degree distribution of the graph using the function degree.distribution.


```{r}
degree(g.er)
```
```{r}
degree.distribution(g.er)

```

```{r}
degree(g.ba)
```
```{r}
degree.distribution(g.ba)

```


### Degree Centrality

Which vertex is most central according to Degree Centrality?

```{r}
plot(g.er,vertex.size=1)
```


```{r}
order(degree(g.er), decreasing=TRUE)
```


```{r}
plot(g.ba,vertex.size=1)
```



```{r}
order(degree(g.ba), decreasing=TRUE)
```

### Closeness

We defined the closeness centrality of a vertex 
v
 as the sum of the distance from 
v
 to all other vertices. To compute the closeness of each vertex, we use:

```{r}
closeness(g.ba)
```

```{r}
order(closeness(g.ba), decreasing = TRUE)
```

```{r}
order(closeness(g.er), decreasing = TRUE)
```

### Betweenness
Betweenness centrality measures how often a vertex is used in the shortest paths. We can compute betweenness using:

```{r}
betweenness(g.er)
```

```{r}
order(betweenness(g.er), decreasing = TRUE)
```

```{r}
order(betweenness(g.ba), decreasing = TRUE)
```

### Practice


```{r}
g3 = graph.formula(A-B, A-C, A-D, B-D, B-E, E-D, C-E)
plot(g3)
```


```{r}
degree(g3)
```



```{r}
degree.distribution(g3)
```

```{r}
closeness(g3)
```

```{r}
betweenness(g3)
```




# Clustering

## Problem statement

Given 3 two-dimension points: A(1,1), B(2,1), C(4,5)

Find two clusters for them.

## Solution

### Randomly choose 2 cluster centers:

C1(2,2) and C2(3,3)


Compute the distance between each point to each center:


```{r}
(A=matrix(c(1,1,2,1,4,5,2,2,3,3),byrow=TRUE, nrow=5))
dist(A)
```
From the matrix, we can see that A, B belong to cluster 1 and C belongs to cluster 2.

### Determine 2 new cluster centers

D1=(1.5, 1), D2=(4,5)



```{r}
(A=matrix(c(1,1,2,1,4,5,1.5,1,4,5),byrow=TRUE, nrow=5))
dist(A)
```














## Single linkage clustering

Given a distance matrix between 5 vectors, find the hierarchy of clusters using Single linkage clustering.

```{r}
A=matrix(c(0,1,7,5,6,
           1,0,4,8,6,
           7,4,0,2,8,
           5,8,2,0,3,
           6,6,8,3,0),nrow=5, byrow=TRUE)
rownames(A)=c("x1","x2","x3","x4","x5")
colnames(A)=c("x1","x2","x3","x4","x5")
A
```


```{r}

A= as.dist(A, diag = TRUE)
h = hclust(A, method="single")
plot(h)

```






## Complete linkage cluster

Given a distance matrix between 5 vectors, find the hierarchy of clusters using Complete linkage clustering.

```{r}
A=matrix(c(0,3,7,8,9,
           3,0,5,7,6,
           7,5,0,2,1,
           8,7,2,0,4,
           9,6,1,4,0),nrow=5, byrow=TRUE)
rownames(A)=c("x1","x2","x3","x4","x5")
colnames(A)=c("x1","x2","x3","x4","x5")
A
```


```{r}

A= as.dist(A, diag = TRUE)
h = hclust(A, method="average")
plot(h)

```





# Visualisation of Social Web Data

## Data

```{r}
load(file = "/cloud/project/Module5.RData")

```


```{r}
tweets = rbind(tweets1, tweets2, tweets3)
head(tweets)
```




## Word cloud

```{r}
library(tm)
```

```{r}
corpus = Corpus(VectorSource(tweets$text)) # create a corpus from tweet text
corpus = tm_map(corpus,  function(x) iconv(x, to='ASCII'))
```

```{r}
tdm = TermDocumentMatrix(corpus, 
 control = list(removePunctuation = TRUE,  
 stopwords =c(stopwords(),"will"),
 removeNumbers = TRUE, 
 tolower = TRUE, 
 stemming=TRUE)) 

```


```{r}
empties = which(colSums(as.matrix(tdm)) == 0)

tdm = tdm[,-empties]

# Convert to a standard R matrix
M = as.matrix(tdm)
```

```{r}
library(wordcloud)
```

```{r}
freqs = rowSums(M)
## remove any words that have count "NA".
#freqs = freqs[!is.na(freqs)]
wordcloud(names(freqs), freqs, random.order=FALSE, min.freq=3)
```

## Principal component analysis


```{r}
 # Word frequencies correspond to row Sums in this tdm.
tdmw = weightTfIdf(tdm)  #note that this function requires a tdm matrix
# you cannot use weigthTfIdf function with a document term matrix.
T = as.matrix(tdmw)

```


```{r}
colours = c(rep("red", nrow(tweets1)), 
rep("blue", nrow(tweets2)), 
rep("green", nrow(tweets3)))
## remove colours associated to empty tweets
colours = colours[-empties]
 
pcaT <- prcomp(t(T))
## plotting 1st and 2nd PC
plot(pcaT$x[,1], pcaT$x[,2], col=colours, pch=16)
```
```{r}
 ## plotting 1st and 3rd PC
plot(pcaT$x[,1], pcaT$x[,3], col=colours, pch=16)
```


```{r}
pcaM <- prcomp(t(sqrt(M)))
 ## plotting 1st and 2nd PC
plot(pcaM$x[,1], pcaM$x[,2], col=colours, pch=16)
```





```{r}
summary(pcaT)$importance[,1:5]
```


```{r}
summary(pcaM)$importance[,1:5]
```


## Multidimensional Scaling


```{r}
 D = dist(t(T))
mdsT <- cmdscale(D, k=2)
 plot(mdsT[,1], mdsT[,2], col=colours, pch=16)
```



```{r}
D = dist(t(M), method = "binary")
 mdsM <- cmdscale(D, k=2)
plot(mdsM[,1], mdsM[,2], col=colours, pch=16)
```


```{r}
CM = M %*% diag(1/sqrt(colSums(M^2)))
D = dist(t(CM), method = "euclidean")^2/2
mdsM <- cmdscale(D, k=2)
plot(mdsM[,1], mdsM[,2], col=colours, pch=16)
```






```{r}
CT = T %*% diag(1/sqrt(colSums(T^2)))
 D = dist(t(CT), method = "euclidean")^2/2
mdsT <- cmdscale(D, k=2)
plot(mdsT[,1], mdsT[,2], col=colours, pch=16)
```












# Text mining: day 4



## Searching Twitter



```{r}
load(file = "/cloud/project/Module4.RData")

```


```{r}
names(kevin_bacon_tweets)
```


```{r}
strsplit(kevin_bacon_tweets$text[1], "[^A-Za-z]+")
```

```{r}
tweet.words = strsplit(kevin_bacon_tweets$text, "[^A-Za-z]+")
```


```{r}
word.table = table(unlist(tweet.words))
```


```{r}
sort(word.table, decreasing=TRUE)[1:20]
```


## Text Mining


```{r}
library("tm")
library("SnowballC")  #required for stemming
```



```{r}
tweet.corpus = Corpus(VectorSource(kevin_bacon_tweets$text))
```
```{r}
tweet.corpus = tm_map(tweet.corpus,
         function(x) iconv(x, to='UTF8', sub='byte')) # for Windows
```

```{r}
tweet.corpus = tm_map(tweet.corpus, function(x) iconv(x, to='ASCII', sub=' ')) # remove special characters
tweet.corpus = tm_map(tweet.corpus, removeNumbers) # remove numbers
tweet.corpus = tm_map(tweet.corpus, removePunctuation) # remove punctuation
tweet.corpus = tm_map(tweet.corpus, stripWhitespace) # remove whitespace
tweet.corpus = tm_map(tweet.corpus, tolower) # convert all to lowercase
tweet.corpus = tm_map(tweet.corpus, removeWords, stopwords()) # remove stopwords
tweet.corpus = tm_map(tweet.corpus, stemDocument) # convert all words to their stems
```


```{r}
tweet.dtm = DocumentTermMatrix(tweet.corpus) # create the DocumentTermMatrix object
tweet.matrix = as.matrix(tweet.dtm) # convert to a matrix
```


```{r}
N = nrow(tweet.matrix)
IDF = log(N/colSums(tweet.matrix > 0))
TF = log(tweet.matrix + 1)
tweet.weighted.matrix = TF %*% diag(IDF)
```



```{r}
w = colSums(tweet.weighted.matrix)
```

```{r}
o = order(w, decreasing = TRUE)[1:20]
colnames(tweet.matrix)[o]
```




# Exercise



## Ex1: write a code to compute the sum from 1 to n


```{r}
sum.to.n = function(n) {
  sum = 0
  x = 1
  while(x <= n) {
    sum = sum + x
    x = x + 1
  }
  return(sum)
}

sum.to.n(100)
```

```{r}
sum.to.n = function(n) {
  return(n*(n+1)/2)
}
sum.to.n(100)
```


